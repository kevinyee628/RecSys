## 实验指标

$$
\begin{array}{c|lcr}
 & \text{系统推荐} & \text{系统未推荐} \\
\hline
喜欢 & 真阳率(TP) & 假阴率(FN) \\
不喜欢 & 假阳率(FP) & 真阴率(TN)\\
\end{array}
$$

#### 1.Accurrary(准确率)

​	准确率代表预测**正确的结果**占**总样本**的百分比

​	缺点：如果样本不均衡，准确率就会失效
$$
Accuracy = \frac{Number\ Of\ Correct\ Predictions}{Total\ Number\ Of\ Predictions}
$$

$$
Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
$$

#### 2.Precision(查准率)

​	**系统推荐的内容中用户喜欢的商品所占比例**
$$
Precision = \frac{TP}{TP+FP}
$$

#### 3.Recall(召回率)

​	**用户喜欢的东西有多少被推荐了**

​	**召回率**是针对我们原来的**样本**而言的，它表示的是**样本中的正例有多少被预测正确**了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。

​	*例如：肿瘤预测中不放过任何一个肿瘤*
$$
Recall = \frac{TP}{TP+FN}
$$



![Precisionrecall](/Users/yee/Downloads/Precisionrecall.svg)

<div align = "center">图 1 precision and recall</div>

#### 4.P-R曲线

​	precision和recall是一对矛盾的度量，基于此，我们引出P-R曲线这个概念，若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者。为了综合考虑precision和recall的性能度量，我们在下面继续提出新的度量方式。

​	“平衡点(Break-Even Point, 简称BEP)”就是这样的一个度量，它是“precision = recall”时的取值。

![image-20211112162332223](/Users/yee/Library/Application Support/typora-user-images/image-20211112162332223.png)

<div align = "center">图 2 P-R曲线</div>

#### 5.F-measure

​	BEP过于简化，所以继续提出F1度量：
$$
F-measure=\frac{2*precision*recall}{precision+recall}
$$

#### 6.ROC(Receiver Operating Characteristic) and AUC(Area under curve)

ROC曲线综合考虑学习器在不同任务下的“期望泛化能力”的好坏，或者说是“一般情况下”泛化性能的好坏，是研究学习器泛化性能的有力工具。

​	真阳率和假阳率比率曲线：最终被推荐但**用户不喜欢**的推荐物品所占比例

​	*Tips：挖掘潜在销售同时又对最小化市场成本感兴趣情况下，ROC曲线要比准确率-召回率曲线更适用。*

![image-20211112161026190](/Users/yee/Library/Application Support/typora-user-images/image-20211112161026190.png)

<div align = "center">图 3 ROC曲线</div>

 	ROC曲线和PR曲线相似，如果另一条ROC曲线完全包住另一条曲线，则证明前者的性能优于后者，但是如果两条曲线有交叉，那么久很难比较出来，此时如果要比较两者的性能，就需要比较ROC曲线下的面积，即AUC。

#### 7.nDCG(Normalized Discounted cumulative gain, 归一化折损累计增益)

​	**nDCG是判断排名质量的指标**

![image-20211115221523436](/Users/yee/Library/Application Support/typora-user-images/image-20211115221523436.png)

<div align = "center">图 4 nDCG计算流程</div>

- **高关联度的结果比一般关联度的结果更影响最终的指标得分**

- **有高关联度的结果出现在更靠前的位置的时候，指标会越高**

- **排名的结果应该与执行的查询（规范化）无关**

  k表示推荐系统中top-k, rel表示第k个物品的相关性或者评分。假设我们共推荐k个电影，rel可以是用户对第i部电影的评分。

  1.累积增益CG

  ​	CG就是将每个推荐结果相关性的分支（分数）累加后作为整个推荐列表的得分
  $$
  CG_{k}=\sum_{i=1}^{k}{rel_i}
  $$

  2.折损累积增益DCG

  ​	CG的一个缺点是没有考虑每个推荐结果处于不同位置对整个推荐效果的影响,例如我们总是希望相关性高的结果应该排在前面。显然,如果相关性低的结果排在靠前的位置会严重影响用户的体验,所以在CG的基础上引入位置影响因素, 即DCG(Discounted Cumulative Gain), 这里指的是**对于排名靠后推荐结果的推荐效果进行“打折处理”**:
  $$
  DCG_k=\sum_{i=1}^{k}\frac{rel_i}{log_{2}{(i+1)}}
  $$

  $$
  DCG_k=\sum_{i=1}^{k}\frac{2^{rel_i}-1}{log_{2}{(i+1)}}
  $$

  3.归一折损累积增益(nDCG)

  ​	不同的推荐列表无法横向比较，所以提出nDCG，将DCG进行归一化处理。
  $$
  IDCG_k=\sum_{i=1}^{\left|REL\right|}\frac{2^{rel_i}-1}{log_{2}{(i+1)}}
  $$
  

  ​	IDCG=最好的DCG，指的是推荐系统为用户返回的最好的推荐列表。

$$
NDCG_k = \frac{DCG_k}{IDCG_k}
$$

#### 8.HR(Hit ratio)

​	分母是所有的测试集合，分子表示每个用户top-K列表中属于测试集合的个数的总和。
$$
HR@K = \frac {Number\ of\ Hits@K}  {GT}
$$

#### 9.RMSE(均方根误差)

​	系统在已知用户-物品对(u，i)的真实评分$r_{ui}$情况下再测试集$\tau$上生成预测评分$r_{ui}$。
$$
RMSE = \sqrt{\frac{1}{\left|\tau\right|}\sum_{(u,i)\in\tau}({\hat r_{ui}-r_{ui}})^2}
$$

#### 10.MAE(平均绝对误差)


$$
MAE=\frac{1}{\tau}\sum_{(u,i)\in\tau}\left|{{\hat r_{ui}-r_{ui}}}\right|
$$

#### 

#### 11.Average Precision

​	AP是PR曲线下面的面积，通常来说一个越好的模型，AP值越高，MAP是多个类别AP的平均值，对每个类的AP再求平均，就得到了MAP的值，MAP的大小在[0,1]区间，越大越好。

![image-20211112162332223](/Users/yee/Library/Application Support/typora-user-images/image-20211112162332223.png)

​																					

<div align = "center">图 3 PR曲线</div>

$$
AveP = \int_{0}^{1} p(r)\, {\rm d}r
$$

其中，$p(k)$表示精准率，$rel(k)$是一个指标函数，如果排名k的项目是相关文档，则等于1，否则等于0.
$$
AveP=\frac{\sum_{k=1}^{n}{P(k)}*{rel(k)}}{number\ of\ relevant\ documents}
$$



#### 12.MAP(Mean Average Precision)

​	MAP计算的是N个用户的平均精度的均值。
$$
MAP = \frac{\sum_{q=1}^{N}{AveP(q)}}{N}
$$

#### 13.Average Recall (AR)

$$
IoU = \frac{Area\ of\ Overlap}{Area\ of\ Union}
$$

​	IoU相当于两个重叠的部分除以两个区域的集合

​	Recall x IoU curve


$$
AR=2 \int_{0.5}^{1}recall(IoU)d(IoU)
$$

#### 14.MAR(Mean Average Recall)

$$
mAR = \frac{\sum_{i=1}^{k}AR_i}{K}
$$



#### 15.MRR(Mean Reciprocal Rank) 

​	平均倒数排名是一种统计量度，用于评估对查询样本生成可能响应列表的任何过程，按正确性概率排序。 查询响应的倒数排名是第一个正确答案的排名的乘法倒数：第一名 1，第二名 1⁄2，第三名 1⁄3，依此类推。 平均倒数排名是查询样本 Q的结果倒数排名的平均值.

​	其中 ${\displaystyle {\text{rank}}_{i}} $指的是第 i 个用户的第一个推荐有效值的排名位置，$Q$表示用户个数

​	平均倒数秩的倒数对应于秩的调和平均数。
$$
MRR = \frac{1}{\left|Q\right|}\sum_{i=1}^{\left|Q\right|}\frac{1}{rank_i}
$$

#### 18.Coverage(覆盖率)

​	覆盖率（Coverage）描述一个推荐系统对物品长尾的发掘能力。最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。

​	假设系统的用户集合为$U$，总物品集合为$I$，推荐系统给每个用户推荐一个长度为N的物品列表$R(u)$:	
$$
coverage = \frac{U_{u\in U }R(u)}{\left|I\right|}
$$
​	可以通过研究物品在推荐列表中出现的次数的分布描述推荐系统**挖掘长尾**的能力。如果这个分布比较平，那么说明推荐系统的覆盖率比	较高，而如果这个分布比较陡峭，说明这个推荐系统的覆盖率比较低。在信息论和经济学中有两个著名的指标可以用来定义覆盖率。

​	1. 信息熵：

​		p(i)表示物品i的流行度除以所有的物品流行度之和。
$$
H = -\sum_{i=1}^{n}p(i)logp(i)
$$
​	2. 基尼系数：
$$
Gini = \frac{1}{n-1}\sum_{j=1}^{n}(2j-n-1)p(i_j)
$$
​		这里，是按照物品流行度p从小到大排序的物品列表中的第j个物品。

​		评测推荐系统是否具有马太效应的简单办法就是使用基尼系数。

#### 16.Diversity(多样性)

​	$R(u)$表示用户$u$的推荐列表
$$
Diversity(R(u)) = \frac{\sum_{i,j\in R(u),i\ne j}1-Sim(i,j)}{\frac{1}{2}\left|R(u)\right|(\left|R(u)\right|-1)}
$$
​	推荐系统整体的多样性可以用如下公式表示：
$$
Diversity = \frac{1}{\left|U\right|}\sum_{u\in U}Diversity(R(u))
$$


#### 17.Novelty(新颖性)

​	$Z_u$表示推荐给用户的n个item集合
$$
Novelty_i = \frac{1}{Z_u-1}\sum_{j\in Z_u}[1-Sim(i,j)],\ i\in Z_u
$$


#### 18.ILD

​	$c_i$代表item

​	$N(u_i)$代表$u_i$的邻居
$$
ILD=\frac{1}{\left|L\right|}\sum_{u_i\in L}\frac{1}{c_i(c_i-1)}\sum_{(v_k,v_j)\in N(u_i)}dist(v_k,v_j).
$$

$$
ILS(R)=\frac{2}{k(k-1)}\sum_{i\in R_j\ne i}\sum_{i\in R}{Sim(i,i)}
$$

​	ILS主要针对单个用户，一般来说ILS值越大，单个用户推荐列表多样性越差，其中，i和j为Item，k为推荐列表长度，sim为相似性度量。

​																					**图 2 ILS公式与意义**

#### 19.Limited Area Under the Curve (LAUC)

在AUC的基础上取前k个item计算面积
$$
AUC = \sum_{i=2}^{m}\frac{(x_i-x_{i-1})*(y_i+y_{i-1})}{2}
$$




![image-20211112145719435](/Users/yee/Library/Application Support/typora-user-images/image-20211112145719435.png)

<div align = "center">图 4</div>



## 参考文献

[1] https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision

[2] https://en.wikipedia.org/wiki/Mean_reciprocal_rank

[3] Schröder G, Thiele M, Lehner W. Setting goals and choosing metrics for recommender system evaluations[C]//UCERSTI2 workshop at the 5th ACM conference on recommender systems, Chicago, USA. 2011, 23: 53.

[4] https://zhuanlan.zhihu.com/p/43068926

[5] https://www.guoyaohua.com/classification-metrics.html#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98

[6] 机器学习 作者：周志华

[7] https://www.jianshu.com/p/1137c51de993

